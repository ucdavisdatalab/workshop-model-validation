```{r ch3-loads, echo=FALSE, warning=FALSE, include=FALSE}
library( "readr" )
library( "lubridate" )
library( "dplyr" )
library( "rsample" )
library( "parsnip" )
library( "workflows" )
library( "workflowsets" )

covid = read_csv( "data/covid.csv" )
covid$DAY_OF_THE_WEEK = factor(covid$DAY_OF_THE_WEEK)
```

# Data splitting systems
To train and validate a model requires that we have in-sample and out-of-sample data, but typically we have just "the data". There are a couple of approaches to separating that data in in-sample and out-of-sample sets: training/validation split or cross-validation. They are often used together.


## Time series data
Our example data are time series, so it is good practice for validation splits to respect the time ordering of the observations. That's because the results of validation are more realistic when they work from a known past to an unknown future.


## Train/testing split
One solution is to reserve some data for validation, and use what is left for training the model. The split can be random or not - for instance, you may hold back the most recent year of data for validation, or you may randomly sample some proportion (e.g., 50%) of the observations to reserve for validation. The `rsample` package provides a functions called `validation_split()` and `validation_time_split()` to split the data into training and testing sets. The difference between the two is that `validation_split()` does a random split and `validation_time_split()` does its splitting by keeping the first part of the data for training and the latter part for testing. The same package provides the functions `testing()` and `training()` to extract the testing set and the training set, respectively, from the split.

For the examples, we will train using the first 50% of the observations and validate using the last 50%.

```{r split_covid_data}
# do a time split on the covid data
covid_split = validation_time_split( covid, prop=0.5 )

# inspect the split object
covid_split

# extract the training set from the split
covid_train = training( covid_split$splits[[1]] )

# inspect the training data
covid_train
```


## Cross-validation
Cross validation (often abbreviated CV) is a kind of repeated training/validation split. The data is broken into several chunks (called "folds"), and one is held out for validation. All of the others are used as a training set, then resulting model is used to predict the response over the held-out fold. The process is iterated until each fold has been held out once. The main benefit of cross validation is that by iterating over the folds, you end up with a prediction for every data point. This can be important when doing a single train/test split would leave too few observations in the test set to draw reliable conclusions for validation.

Each training/validation split may be random or may take data that are grouped according to some meaningful value. For instance, time-series data may be best analyzed by holding out contiguous blocks of observations.

We will use cross-validation on the training set to help build the models, before we validate them using the test set. As before, the `rsample` package provides convenient functions to create cross-validation splits that play nicely with other parts of the `tidymodels` system. Here, the CV folds aren't using contiguous time blocks, which is a shortcoming. We're doing it this way because the `tidymodels` tools for creating and using CV folds don't provide that functionality. To try to write the loops that would do the job properly is beyond the scope of this workshop.

```{r covid_cv}
# create ten cross-validation folds on the training set
covid_cv = vfold_cv( covid_train, v=10 )

# inspect the CV folds
covid_cv
```


## Combinations
There are times when cross-validation and training/testing validation should be used together. For instance, when cross-validation is used for exploratory analysis and model selection, then validation should be done using new data that was never previously part of the estimation. That's how we are handling the examples in this workshop.





