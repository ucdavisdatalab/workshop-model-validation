```{r ch3-load-data, include=FALSE}
data( cars )
```

# Data splitting systems
To train and validate a model requires that we have in-sample and out-of-sample data, but typically we have just "the data". There are a couple of approaches to separating that data in in-sample and out-of-sample sets: training/validation splits or cross-validation.

## Train/validation split
One solution is to reserve some data for validation, and use what is left for training the model. The split can be random or not - for instance, you may hold back the most recent year of data for validation, or you may randomly sample some proportion (e.g., 30%) of the observations to reserve for validation.


## Cross-validation
Cross validation is a kind of repeated training/validation split, which is iterated until all of the data has been used for testing. Each training/validation split may be random or may take data that are grouped according to some meaningful value. For instance, time-series data may be best analyzed by holding out contiguous blocks of observations.

Here, I'll demonstrate doing thre-fold cross-validation on the car stopping distance data. I'll split the data into three folds and use those divisions to estimate three models. Each model uses two folds as training data and one as validation data.

```{r three-fold-cv, echo=FALSE, cache=TRUE}
# standard plotting parameters
ylim = range( cars$dist )
xx = seq( min(cars$speed), max(cars$speed), length.out=200 )

# randomly create three folds
fold = sample(c("black", "purple", "darkorange"), size=nrow(cars), replace=TRUE)
plot( cars, col=fold, bty='n', pch=16, ylim=ylim )

# divide the plotting window into two panels
layout( matrix(1:2, 1, 2) )

for (holdout in c("black", "purple", "darkorange")) {
  # create a regression model on the training folds
  cv3 = lm( dist ~ poly(speed, 2), data=cars[fold != holdout, ])
  
  # plot the training data with the estimated model
  plot(cars[ fold != holdout, ],
       bty='n',
       col=fold[ fold != holdout],
       pch=16,
       ylim=ylim )
  lines( x=xx, y=predict(cv3, data.frame(speed=xx)))
  
  # plot the held out data with the estimated model
  plot( cars[fold == holdout, ],
        bty='n',
        col=holdout,
        pch=16,
        ylim=ylim )
  lines( x=xx, y=predict(cv3, data.frame(speed=xx)), lty=2)
}
```

Some things to note are that the estimated line of best fits are different for each iteration of the CV, and that the predictions (in the right hand panels) are generally less accurate than the fitted values (in the left hand panels).


### Combining the two
There are times when cross-validation and training/testing validation should be used together. For instance, when cross-validation is used for exploratory analysis and model selection, then validation should be done using new data that was never previously part of the estimation.





