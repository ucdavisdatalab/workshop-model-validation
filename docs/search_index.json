[["index.html", "Model validation for applied data science Overview", " Model validation for applied data science Wesley Brooks 2021-11-10 Overview YOUR DESCRIPTION, LEARNING GOALS, PREREQUISITES, ETC "],["beginnings.html", "1 Beginnings 1.1 Examples 1.2 Typical modeling assumptions", " 1 Beginnings Validating a model is the process of testing it to decide whether the assumptions that are built into the model’s construction are valid. Typically, any kind of statistical method entails some assumptions about the data. That usually means assumptions about the population from which the data were sampled. It is generally impossible to confirm an assumption like, “samples from the population are independent” or, “the population is normally distributed”. Instead, we follow the scientific method of proposing some phenomenon that is bound to happen if the assumption is true, and then if that phenomenon does not occur we have evidence against the assumption. Statistical modeling and model validation begin no later than your first look at your data. This is when you can begin to ask questions like, - “Are there limits to the range of the response variable?” - “What is the distribution of the response variable?” - “Based on my prior knowledge, am I able to anticipate the structure of the model?” If you have answers to these, then you are well on your way to creating a great model. With that in mind, I’m going to start this workshop by introducing examples of data. 1.1 Examples Let’s load the data for a few examples of data that we’ll be looking at today. First, though, we need to load some packages and set the path to use for loading data. library( &quot;dplyr&quot; ) library( &quot;lubridate&quot; ) library( &quot;readr&quot; ) library( &quot;gbm&quot; ) # set the root path for loading data. data_path = &quot;https://raw.githubusercontent.com/ucdavisdatalab/workshop-model-validation/master/data/&quot; 1.1.1 Stopping distance The dataset cars is built into R and can be loaded by the command data(cars). It is a record of fifty measurements from 1930 of stopping distances for cars travelling at different speeds. We are going to select the functional form of a model that relates the stopping distance to the speed. Here’s a plot of the data: # load the data data( cars ) # plot the cars data plot( cars, bty=&#39;n&#39; ) We have some relevant prior knowledge: speed and distance are both positive, zero speed implies zero stopping distance, and kinetic energy is proportional to the square of speed. Both speed and distance are continuous quantities, and the vertical spread of the points tends to get wider as the speed increases. 1.1.2 Breast cancer survival We will also use the data set gbsg (which is built into R’s survival package) for an analysis of the factors affecting the five-year survival of women in the German Breast Cancer Study Group (GBSG). Our goal will be to create a model of how the available variables influence the five year recurrence free survival of women who met the study criteria. Let’s import the data and generate the five year survival variable: # import the gbsg data gbsg = read_csv( url(paste0(data_path, &quot;gbsg.csv&quot;)) ) # make a factor of the grade gbsg$grade = factor( gbsg$grade ) # generate the variable `fys`, to indicate five year recurrence-free survival: gbsg$fys = with(gbsg, ifelse( rfstime &gt; 365*5, 1, ifelse(status == 0, NA, 0))) # plot the data plot( gbsg ) The five year recurrence-free survival outcome is binary, so anticipate using a generalized linear model with binomial response (logistic regression, for example). The pair plot of all the data has some interesting features in the er, pgr, and nodes variables. Let’s take a closer look at those, specifically. # make a plot of the `er`, `pgr`, and `nodes` variables, with `rfstime`. plot( gbsg[, c(&quot;er&quot;, &quot;pgr&quot;, &quot;nodes&quot;, &quot;rfstime&quot;)]) It looks like er, pgr, and nodes are all non-negative, with patterns that cluster near zero and have progressively fewer data points at greater values. These variables are, respectively, the concentrations of estrogen receptors, the concentration of progesterone receptors, and the number of positive lymph nodes. (Subjects in the study were women with at least one positive lymph node biopsy, so none had zero positive nodes.) If we try to fit a linear trend to these data, the few observations with extremely high concentrations of receptors or many positive nodes will be influential outliers, while any meaningful detail near zero will be lost. In order to avoid those pitfalls, a log() or sqrt() transformation may be appropriate. Here, I’ll use a log() transformation on nodes, and a sqrt() transformation for the other two variables, in order to avoid problems trying to calculate log(0) for those women who had no progesterone or estrogen receptors. # plot the square-root transformes er, pgr, and nodes variables, with rfstime: with( gbsg, plot( data.frame(sqrt(er), sqrt(pgr), sqrt(nodes), rfstime))) Note how the outliers have been brought back toward the mass of data, and the detail at the low end has been expanded. A bit of work on the front end can avoid really difficult problems in the analysis and validation later. 1.1.3 COVID-19 hospital admissions The third data set that we’ll use for illustration is a time series of daily hospital admissions for COVID-19. Our goal will be to create a model that can predict the number of people who will be admitted to the hospital for COVID-19 in the next day. I like to begin with a pair plot of all the data, but here we have 81 columns, which is too many to fit on a pair plot. Instead, we’ll look at the time series. # import the covid dataset covid = read_csv( url( paste0(data_path, &quot;covid.csv&quot;)) ) # plot the admissions time series with( covid, plot(date, COVID_NEW_ADM_CNT, type=&#39;l&#39;) ) The count of daily admissions is a nonnegative integer. It appears that there is a lot more variability in the admissions count when the recent rate of admissions is near its peaks than when it is near zero. There isn’t a theoretical maximum for daily admissions (though of course there would be a practical maximum, if the hospital became overwhelmed). And we can reasonably assume that each admission was an independent decision. To a statistician, those facts all point toward trying to assume that the counts follow a Poisson distribution. There is another type of independence to consider here: is the number of daily admissions independent from the number of admissions the day before? That kind of independence doesn’t mean you pretend that there’s no information in the most recent counts - instead, it means that the short-term fluctuations are as likely to be above the trend as below it. Without that independence, we would say that the data are autocorrelated. And we can test for autocorrelation: # plot the autocorelation function of the diff-1 covid admissions acf( diff(covid$COVID_NEW_ADM_CNT, 1) ) You can ignore the first line of the chart - it is there to show how long a line is drawn to show perfect correlation at zero time lag. The salient feature of this plot is that the third line points downward, and crosses the dashed blue line. There is apparently a tendency for the count to decrease the day after it increases, and vice versa. This may or may not be due to some trend in the covariates - we’ll check again after accounting for the explanatory variables. 1.2 Typical modeling assumptions There are three general assumptions that underlie most statistical models Observations are usually assumed to be independent of each other (or assume some specific form of dependence). The expectation of the response variable is a function of the input variables. \\(\\mathbb{E}(Y) = f(X)\\). The response variable has some distribution, like \\(Y\\) is Normal, or binomial, or Poisson. Validating these assumptions also requires that the function \\(f(\\cdot)\\) and the response distribution can be learned from the data. So, how can we test (validate) these assumptions? A key problem is that they generally can’t be tested from within a fitted model. The way statistical estimation works is that you assume the form of a model, and then find the parameters that best match the data and the assumption. There is a circularity to the logic of estimating a model that requires some assumption about the data and then using that same data to check whether the assumptions are violated. Testing requires applying the model to new data in order to see if the predictions match the truth. "],["model-types.html", "2 Model types 2.1 Linear regression 2.2 Logistic regression 2.3 Random forest regression", " 2 Model types The data examples each require a different kind of model, which will allow us to demonstrate several different instances of model validation. 2.1 Linear regression We will use linear regression to create a model that relates a car’s speed to the distance required for stopping the car. Linear regression can use transformed versions of the inputs and outputs, and we will test various combinations of transformations to decide which one is the best. The functional form of a linear regression model looks like \\[ y = X \\beta + \\varepsilon \\] where \\(X\\) are the (possibly transformed) covariates, \\(\\beta\\) are the regression coefficients, \\(\\varepsilon\\) are residual errors, and \\(y\\) is the (possibly transformed) response variable. Linear regression assumes that the observations are independent, that the functional form accurately describes the relationship between inputs and response, and that the residual errors are follow a normal distribution with mean zero and constant variance. 2.2 Logistic regression Logistic regression is a method of estimating the probability of an event occurring. An event is a discrete occurrence that either happens or does not. For example, a subject surviving for at least five years after enrollment in a clinical trial is one example of an event - it either happens or does not. Logistic regression assumes that each event has a binomial distribution where the probabolity of occurrence is \\(p\\), and \\[p = \\exp{(X \\beta)} / \\{ 1 + \\exp{(X \\beta)} \\}.\\] As hinted above, validating a logistic regression model would involve checking that the observations are independent from a binomial distribution, and that the probability of each event is as above. 2.3 Random forest regression Random forest regression models are an entirely different class of model. There is no linear function like the \\(X \\beta\\) of logistic regression (or, indeed, linear regression). Instead, the \\(f( X )\\) part of the model is constructed from an ensemble of regression trees. Each regression tree is a collection of discrete branches that ultimately lead to conclusions at the end of the chain. Consider the stopping distance example again. One regression tree might include logic like, IF speed &gt; 8 AND speed &lt;= 12 THEN stoping distance is 20 feet. As a forest is made of many trees, a random forest model is made of many regression trees, each of which uses a random portion of the total training data. In fact, the GBM models I’ll use here include some additional “gradient boosting” logic that takes them beyond a basic random forest model, but that I’m going to mostly ignore for our purposes today. The response variable for the random forest model is the number of new COVID-19 admissions on a given day. I’m treating this count as if it follows a Poisson distribution. Thus, the assumptions that must be validated for this model are the relationship \\(\\mathbb{E}(Y) = f( X )\\), and the independence and Poisson distribution of the counts. "],["data-splitting-systems.html", "3 Data splitting systems 3.1 Train/validation split 3.2 Cross-validation", " 3 Data splitting systems To train and validate a model requires that we have in-sample and out-of-sample data, but typically we have just “the data”. There are a couple of approaches to separating that data in in-sample and out-of-sample sets: training/validation splits or cross-validation. 3.1 Train/validation split One solution is to reserve some data for validation, and use what is left for training the model. The split can be random or not - for instance, you may hold back the most recent year of data for validation, or you may randomly sample some proportion (e.g., 30%) of the observations to reserve for validation. 3.2 Cross-validation Cross validation is a kind of repeated training/validation split, which is iterated until all of the data has been used for testing. Each training/validation split may be random or may take data that are grouped according to some meaningful value. For instance, time-series data may be best analyzed by holding out contiguous blocks of observations. Here, I’ll demonstrate doing thre-fold cross-validation on the car stopping distance data. I’ll split the data into three folds and use those divisions to estimate three models. Each model uses two folds as training data and one as validation data. Some things to note are that the estimated line of best fits are different for each iteration of the CV, and that the predictions (in the right hand panels) are generally less accurate than the fitted values (in the left hand panels). 3.2.1 Combining the two There are times when cross-validation and training/testing validation should be used together. For instance, when cross-validation is used for exploratory analysis and model selection, then validation should be done using new data that was never previously part of the estimation. "],["validating-the-models-functional-form.html", "4 Validating the model’s functional form 4.1 Posit a model 4.2 Validate the assumptions", " 4 Validating the model’s functional form We can validate the model’s functional form by making predictions and comparing them to the truth. If the estimated mean function is valid, then the observed should be in line with predictions. Of course, there will stil be some error, but the predictions should be accurate on average. 4.1 Posit a model To posit a model here means to select a model form and distribution, and apply them to a piece of estimation software. Most often the model form is a generalized linear model (which includes linear regression as a special case), but we are also looking at random forest-type models, which are not linear in their inputs. 4.1.1 Model form Selecting a model form includes deciding which covariates have a predictive relationship with the response, and whether their relationship is linear, linear following some transformation, or should be modeled more flexibly, e.g. with a tree-based approach. 4.1.2 Model distribution Knowing the allowable range of the response will help you to decide what distribution to specify in fitting a model. Here are some common, crucial decision points: If the response is in the form of “number of events out of some number of tries” (e.g. a basketball player’s free throw percentage is the number of made free throws out of tries), then a binomial distribution may be appropriate. This is also the case when the number of “tries” is one - as in our example of five years survival for women with breast cancer. Here, the “event” is “the woman survives five years after diagnosis”, and each person in the study gets one try. If the response is a count of independent events, with no “out of” tries, then it may be Poisson distributed. If the data are continuous with no bounds then they may be normally distributed. If the data are continuous, but have a defined lower bound at zero then they may follow a Gamma distribution. 4.2 Validate the assumptions Having posited a model, we may check whether the model generates fits and predictions that agree with the assumptions. Let’s do this for our examples. 4.2.1 Stopping distamce example One common task in statistical modeling is model selection. Adding more predictors to a regression model will always improve the fit to the training data, but it often doesn’t help the accuracy of prediction (and can even cause predictions to be less accurate.) Since a statistical model is all about identifying consistent or reliable trends in the data, we want to be sure that the relationships we observe in the training data are still there in new data. In the context of model selection, that means picking the model that does best at predicting left out data One way to do that is to reserve some validation data that is not used until the very end of a data analysis, when it may be used to confirm which model does best on the training data. Here, we’ll do that by using six-fold cross validation but leaving out two folds (1/3 of the data) for validation. I’m going to demonstrate creating a bunch of candidate models with different functional relationships between the speed and stopping distance. This kind of exploration is something you’ll often do when creating a regression model, but it can lead to picking a model that best matches the available training data, rather than one that works well when applied to new observations. That’s why it is important to have some validation data that was never used in the selection and estimation process. set.seed( 20211109 ) nfold = 6 folds = sample( 1:nfold, size=nrow(cars), replace=TRUE ) train = folds &lt;= 4 preds = list( rep( NA, nrow(cars)), rep( NA, nrow(cars)), rep( NA, nrow(cars)), rep( NA, nrow(cars)), rep( NA, nrow(cars)), rep( NA, nrow(cars)) ) for ( i in 1:4 ) { indx = folds!= i &amp; train # Estimate a model for the stopping distance data lm1 = lm( dist ~ speed, data=cars[ indx, ] ) lm2 = lm( dist ~ speed + I(speed^2), data=cars[ indx, ] ) lm3 = lm( dist ~ speed + I(speed^2) + I(speed^3), data=cars[ indx, ] ) lm4 = lm( dist ~ 0 + speed, data=cars[ indx, ] ) lm5 = lm( dist ~ 0 + speed + I(speed^2), data=cars[ indx, ] ) lm6 = lm( dist ~ 0 + speed + I(speed^2) + I(speed^3), data=cars[ indx, ] ) preds[[1]][ folds == i ] = predict( lm1, cars[ folds==i, ] ) preds[[2]][ folds == i ] = predict( lm2, cars[ folds==i, ] ) preds[[3]][ folds == i ] = predict( lm3, cars[ folds==i, ] ) preds[[4]][ folds == i ] = predict( lm4, cars[ folds==i, ] ) preds[[5]][ folds == i ] = predict( lm5, cars[ folds==i, ] ) preds[[6]][ folds == i ] = predict( lm6, cars[ folds==i, ] ) } # calculate the mean squared error of the models mean( (preds[[1]] - cars$dist)**2, na.rm=TRUE ) ## [1] 238.7035 mean( (preds[[2]] - cars$dist)**2, na.rm=TRUE ) ## [1] 261.9381 mean( (preds[[3]] - cars$dist)**2, na.rm=TRUE ) ## [1] 287.3411 mean( (preds[[4]] - cars$dist)**2, na.rm=TRUE ) ## [1] 250.8836 mean( (preds[[5]] - cars$dist)**2, na.rm=TRUE ) ## [1] 252.3792 mean( (preds[[6]] - cars$dist)**2, na.rm=TRUE ) ## [1] 274.4756 Now I’ll compare the three candidate models that had the lowest cross-validation error over the training data, to see which is most accurate on the validation data. # The first, fourth, and fifth candidate models were the most accurate on the # training data. Let&#39;s see how they compare on the validation data. ## first, estimate models over folds 1-4: lm1 = lm(dist ~ speed, data=cars[ folds &lt;= 4, ]) lm4 = lm(dist ~ 0 + speed, data=cars[ folds &lt;= 4, ]) lm5 = lm(dist ~ 0 + speed + I(speed^2), data=cars[ folds &lt;= 4, ]) # now calculate the mean squared error in predictions on folds 5-6 mean( (predict(lm1, cars)[ folds &gt; 4 ] - cars$dist[ folds &gt; 4 ])**2 ) ## [1] 252.5206 mean( (predict(lm4, cars)[ folds &gt; 4 ] - cars$dist[ folds &gt; 4 ])**2 ) ## [1] 300.9768 mean( (predict(lm5, cars)[ folds &gt; 4 ] - cars$dist[ folds &gt; 4 ])**2 ) ## [1] 227.8891 We will prefer model lm5, the one with the formula dist ~ 0 + speed + I(speed^2). 4.2.2 Breast cancer survival example No model selection this time. We have a model formula and must check whether the predicted probabilities accurately describe the observed events. To begin, we need to use cross-validation to get predicted probabilities of five year survival for each subject in the study. nfold = 5 fold = sample( 1:nfold, size=nrow(gbsg), replace=TRUE ) pred_gbsg = numeric( nrow(gbsg) ) for ( i in 1:nfold ) { # Estimate a model for the stopping distance data glm1 = glm( fys ~ age + meno + size + grade + log(nodes) + sqrt(pgr) + sqrt(er) + hormon, data=gbsg[ fold != i, ], family=&#39;binomial&#39; ) # get predictions for the left-out CV fold pred_gbsg[ fold == i ] = predict( glm1, gbsg[ fold == i, ], type=&#39;response&#39; ) } Each person’s outcome is binary - they achieve five years of recurrence free survival or not. It is difficult to discern whether a binary outcome is consistent with its predicted probability, because ther is very little information in a single binary outcome. So to compare the predicted probabilities to the observed frequencies, we will put each subject into one of twenty bins based on their predicted probability of five year survival. For this data, each of those bins contains about forty subjects. Then we can check whether the frequency of five year survival of people in a bin matches the predicted probability of five year survival for that bin. This requires a bit of coding, but we will get through it, I promise: # divide the predictions into twenty bins (nineteen breakpoints) q_gbsg = quantile( pred_gbsg, seq(0.05, 0.95, length.out=19) ) # add 0 and 1 as the extremes of the probabilities q_gbsg = c(0, q_gbsg, 1) # create some variables to hold the observed, expected five-year survival p_gbsg = numeric(20) fys_gbsg = numeric(20) tot_gbsg = numeric(20) # calculate expected and observed five-year survival in each bin for (i in 1:20) { # identify which predictions are within this interval indx = pred_gbsg &gt; q_gbsg[[i]] &amp; pred_gbsg &lt;= q_gbsg[[i+1]] # calculate the mean probability within this interval p_gbsg[[i]] = mean(pred_gbsg[ indx ], na.rm=TRUE) # calculate the number of subects in this interval who survived five years fys_gbsg[[i]] = sum( gbsg$fys[ indx ], na.rm=TRUE ) # calculate the number of subjects in this interval tot_gbsg[[i]] = sum( !is.na(gbsg$fys)[ indx ] ) } # plot the predicted probability within each bin # vs. the observed frequency of five year survival plot( p_gbsg, fys_gbsg / tot_gbsg, xlab = &quot;predicted probability of FYS&quot;, ylab = &quot;observed frequency of FYS&quot;) # add the line that would show a perfect match between observed and expected. abline(0, 1) The observed frequency of five year survival tends to increase with the predicted probability, but there is a pretty large amount of variation around the trend. We will see later how to decide if that variation is what we’d expect from a binomial distribution. 4.2.2.1 COVID-19 admissions example Recall that one of our first validation tasks for the COVID-19 admissions data was to check whether the time series was autocorrelated. We concluded that there was not an important amount of autocorrelation, and so we will use modeling methods that treat the data as independent. Despite that, when the data are a time series, it is good practice for validation splits to respect the time ordering of the observations. After all, you need to be concerned about independence of the predictors as well as the response, and it is simply more realistic to validate the model with a known past and unknown future. That means setting a split date, then training over past data and predicting future observations. You can still use cross-validation for time-series data - it is called walkforward validation, where each fold is a contiguous temporal block. Here, though, we use a single train/test split: train a model using all data prior to July, 2021 and then use it to predict COVID admissions since then. set.seed(20211108) # create a variable for the one-day ahead admissions: covid$D1_admissions = lead(covid$COVID_NEW_ADM_CNT, 1) # remove any rows that have NA values covid = covid[ rowSums( is.na(covid)) == 0, ] # establish the split date and use it to define a training set: split = &quot;2021-07-01&quot; train = covid$date &lt; split # use CV on the training data to decide which variables to use in the models screen = gbm( D1_admissions ~ ., data=covid[ train, -1], n.trees=3000, distribution=&quot;poisson&quot;, interaction.depth=5, n.minobsinnode=3, shrinkage=0.0025, cv.folds=5 ) # show the most influential variables for predicting COVID admissions: my_summary = summary(screen, plotit=FALSE, n.trees = gbm.perf(screen, plot.it=FALSE) ) print(my_summary) ## var rel.inf ## POSITIVITY_RATE_MEAN POSITIVITY_RATE_MEAN 21.15992984 ## COVID_MED_SURG_NO_HFNC_CNT_M COVID_MED_SURG_NO_HFNC_CNT_M 14.83983283 ## COVID_ACTIVE_DNR_CNT_M COVID_ACTIVE_DNR_CNT_M 6.80277776 ## COVID_PT_CNT_MIDNIGHT_MEAN COVID_PT_CNT_MIDNIGHT_MEAN 4.60795378 ## DAY_OF_THE_WEEK DAY_OF_THE_WEEK 3.85556257 ## COVID_NEW_ADM_MEAN COVID_NEW_ADM_MEAN 3.84604649 ## INDX_UCDH_POSITIVITY_RATE INDX_UCDH_POSITIVITY_RATE 3.76074110 ## INDX_UCDH_POS_PT_CNT INDX_UCDH_POS_PT_CNT 3.61568817 ## COVID_MED_SURG_CNT_M COVID_MED_SURG_CNT_M 3.02052191 ## COVID_MALE_CNT_M COVID_MALE_CNT_M 2.98958324 ## OUTREACH_POSITIVITY_MEAN OUTREACH_POSITIVITY_MEAN 2.62827320 ## OUTREACH_POSITIVITY_RATE OUTREACH_POSITIVITY_RATE 1.93222982 ## COVID_DISCHARGE_STD COVID_DISCHARGE_STD 1.75397412 ## COVID_ICU_VENT_PCT_M COVID_ICU_VENT_PCT_M 1.53349524 ## COVID_ICU_NO_VENT_PCT_M COVID_ICU_NO_VENT_PCT_M 1.40921236 ## COVID_PT_CNT_M COVID_PT_CNT_M 1.06311554 ## TESTED_PT_CNT_MEAN TESTED_PT_CNT_MEAN 1.02452494 ## COVID_PT_CNT_MIDNIGHT_STD COVID_PT_CNT_MIDNIGHT_STD 0.97379651 ## COVID_ICU_NO_VENT_CNT_M COVID_ICU_NO_VENT_CNT_M 0.96460797 ## COVID_POS_PT_CNT_EVER COVID_POS_PT_CNT_EVER 0.94551837 ## COVID_AVG_AGE_M COVID_AVG_AGE_M 0.89209631 ## COVID_DISCHARGE_PCT_STD COVID_DISCHARGE_PCT_STD 0.86543726 ## OUTREACH_POSITIVITY_SLOPE OUTREACH_POSITIVITY_SLOPE 0.81215037 ## COVID_NEW_ADM_CNT COVID_NEW_ADM_CNT 0.80256067 ## COVID_MED_SURG_HFNC_CNT_M COVID_MED_SURG_HFNC_CNT_M 0.75407158 ## OUTREACH_POSITIVITY_STD OUTREACH_POSITIVITY_STD 0.69702188 ## INDX_UCDH_TEST_PT_CNT INDX_UCDH_TEST_PT_CNT 0.64150845 ## TESTED_PT_CNT_SLOPE TESTED_PT_CNT_SLOPE 0.63897940 ## COVID_ACTIVE_DNR_PCT_M COVID_ACTIVE_DNR_PCT_M 0.60346285 ## COVID_PT_CNT_MIDNIGHT_SLOPE COVID_PT_CNT_MIDNIGHT_SLOPE 0.54075889 ## COVID_NEW_ADM_STD COVID_NEW_ADM_STD 0.50110912 ## OUTREACH_TEST_CNT_MEAN OUTREACH_TEST_CNT_MEAN 0.46501740 ## OUTREACH_POS_TEST_CNT OUTREACH_POS_TEST_CNT 0.43046780 ## POSITIVITY_RATE_SLOPE POSITIVITY_RATE_SLOPE 0.42406516 ## HOSPITAL_CENSUS_M HOSPITAL_CENSUS_M 0.41996645 ## COVID_MED_SURG_HFNC_PCT_M COVID_MED_SURG_HFNC_PCT_M 0.39540548 ## COVID_ICU_CNT_M COVID_ICU_CNT_M 0.38065354 ## COVID_MED_SURG_NO_HFNC_PCT_M COVID_MED_SURG_NO_HFNC_PCT_M 0.38051158 ## COVID_DISCHARGE_PCT_SLOPE COVID_DISCHARGE_PCT_SLOPE 0.37000680 ## COVID_ICU_VENT_CNT_M COVID_ICU_VENT_CNT_M 0.36308152 ## COVID_MALE_PCT_M COVID_MALE_PCT_M 0.35621454 ## COVID_DISCHARGE_MEAN COVID_DISCHARGE_MEAN 0.35605100 ## OUTREACH_TEST_CNT_SLOPE OUTREACH_TEST_CNT_SLOPE 0.34700125 ## COVID_DISCHARGE_PCT_MEAN COVID_DISCHARGE_PCT_MEAN 0.34611494 ## INDX_POS_PT_ALL_ADM_PCT INDX_POS_PT_ALL_ADM_PCT 0.32505437 ## COVID_DISCHARGE_CNT COVID_DISCHARGE_CNT 0.32031917 ## INDX_POS_PT_NEW_ADM_PCT INDX_POS_PT_NEW_ADM_PCT 0.31225111 ## TESTED_PT_CNT_STD TESTED_PT_CNT_STD 0.29910087 ## POSITIVITY_RATE_STD POSITIVITY_RATE_STD 0.28425817 ## INDX_POS_PT_NEW_ADM_CNT INDX_POS_PT_NEW_ADM_CNT 0.27607528 ## COVID_DISCHARGE_SLOPE COVID_DISCHARGE_SLOPE 0.26998993 ## COVID_DISCHARGE_PCT COVID_DISCHARGE_PCT 0.25148475 ## INDX_POS_PT_IN_HOUSE_D6_CNT INDX_POS_PT_IN_HOUSE_D6_CNT 0.24820349 ## OUTREACH_TEST_CNT OUTREACH_TEST_CNT 0.24039423 ## INDX_POS_PT_ALL_ADM_CNT INDX_POS_PT_ALL_ADM_CNT 0.22543487 ## COVID_NEW_ADM_SLOPE COVID_NEW_ADM_SLOPE 0.20798090 ## OUTREACH_TEST_CNT_STD OUTREACH_TEST_CNT_STD 0.18292958 ## COVID_RULE_OUT_PT_CNT_M COVID_RULE_OUT_PT_CNT_M 0.17851018 ## INDX_POS_PT_IN_HOUSE_D7_PCT INDX_POS_PT_IN_HOUSE_D7_PCT 0.16370110 ## DISCH_MED_SURG_HFNC_CNT DISCH_MED_SURG_HFNC_CNT 0.15038246 ## INDX_POS_PT_IN_HOUSE_D6_PCT INDX_POS_PT_IN_HOUSE_D6_PCT 0.13339931 ## DISCH_ICU_VENT_CNT DISCH_ICU_VENT_CNT 0.12530976 ## DISCH_MED_SURG_NO_HFNC_CNT DISCH_MED_SURG_NO_HFNC_CNT 0.11078148 ## DISCH_ICU_NO_VENT_CNT DISCH_ICU_NO_VENT_CNT 0.10247327 ## INDX_POS_PT_IN_HOUSE_D7_CNT INDX_POS_PT_IN_HOUSE_D7_CNT 0.05787682 ## COVID_NEW_ADM_FLAG COVID_NEW_ADM_FLAG 0.02298892 # use the top twenty variables from the screening model f = paste0(&quot;D1_admissions ~ &quot;, paste( my_summary$var[1:20], collapse=&quot; + &quot;)) # Estimate a model for the covid admissions data boost = gbm( formula(f), data=covid[ train, ], n.trees=3000, distribution=&quot;poisson&quot;, interaction.depth=5, n.minobsinnode=3, shrinkage=0.0025, cv.folds=5 ) # make predictions from the covid admissions model pred_covid = predict(boost, covid, type=&#39;response&#39;)[ !train ] ## Using 1527 trees... # plot the predicted vs. observed plot(pred_covid, covid$D1_admissions[!train], xlab=&quot;prediction&quot;, ylab=&quot;actual future admissions&quot;) abline(a=0, b=1) The predictions are clearly somewhat muted relative to the actual future admissions, but they do capture the trend and when we look at the admissions from the perspective of the assumed Poisson distribution, we will see that the wider errors toward the right of the plot are expected. "],["validating-the-models-distributional-form.html", "5 Validating the model’s distributional form 5.1 Posit a distribution 5.2 Validate your choice", " 5 Validating the model’s distributional form Assuming a form of distribution for the data is not only about allowing the model estimation software to run. It it also necessary before we can create confidence intervals for our predictions. First we will posit a particular distribution for the responses, and then we will validate it. The validation step means that we will reject the posited distribution if it doesn’t agree with the observed data. 5.1 Posit a distribution Recall that we already did this for the examples. This work proceeds by intuition gained through experience, but is often not particularly difficult. We already talked about the allowable range of the responses, and how to interpret those for the mean. 5.2 Validate your choice Compare the difference between the predicted and observed values to what you assume you would see, based on the posited distribution. Let’s look at the examples. 5.2.1 Stopping distance I’ve said that the stopping distance example should use a normal distribution, so let’s look now to see how well the model matches that distribution. Now we can look at the in-sample error distribution, since unlike the mean, estimating the linear model doesn’t enforce the response distribution. # estimate a model for stopping distance stop = lm( dist ~ speed + I(speed^2) + 0, data=cars ) # plot the four diagnostic plots on a 2x2 grid: layout(matrix(1:4, 2, 2)) plot(stop) The diagnostic plots here are suggesting some errors of the linear regression modeling assumptions. The fan shape of the Residuals vs. Fitted plot, and the increasing trend in the Scale-Location plot indicate that the spread of residuals is greater when the fitted values are greater. The points lying above the diagonal of the Q-Q plot suggest that the residuals don’t follow a normal distribution. The model fails validation -let’s look at why. # plot the best fit line and confidence/predictive intervals ## to begin, define the inputs for the annotations xx = data.frame( speed=seq(0, 25, length.out=200) ) ## now plot the data points plot(cars) ## add the best-fit line lines(xx$speed, predict(stop, xx)) ## add lines to show the confidence interval for the best-fit line lines(xx$speed, predict(stop, xx, interval=&quot;confidence&quot;)[, 2], lty=2, col=&#39;red&#39;) lines(xx$speed, predict(stop, xx, interval=&quot;confidence&quot;)[, 3], lty=2, col=&#39;red&#39;) ## add lines to show a prediction interval for new data points lines(xx$speed, predict(stop, xx, interval=&quot;prediction&quot;)[, 2], lty=3, col=&#39;darkorange&#39;) lines(xx$speed, predict(stop, xx, interval=&quot;prediction&quot;)[, 3], lty=3, col=&#39;darkorange&#39;) The plot reveals obvious problems: the prediction intervals include negative distances, and the model doesn’t account for the pattern where the stopping distances are more variable at greater speeds. So, the distribution assumed by the model doesn’t match the distribution of the data. One way of addressing these problems is to implement a transformation that will make the relationship linear. Either a log or a square root transformation could work here, and the square root fits better with the model form that we’ve assumed. So let’s implement it: # take the square root of both sides of the model formula cars_xfrm = lm( sqrt(dist) ~ sqrt(speed) + speed + 0, data=cars ) # plot the diagnostic plots on a 2x2 grid layout( matrix(1:4, 2, 2)) plot(cars_xfrm) The Q-Q plot is greatly improved over the first version, which says that the residuals more closely match the assumption of normality. # plot the dat points plot(cars) ## add the best-fit line lines(xx$speed, predict(cars_xfrm, xx)^2) ## add lines to show the confidence interval for the best-fit line lines(xx$speed, predict(cars_xfrm, xx, interval=&quot;confidence&quot;)[, 2]^2, lty=2, col=&#39;red&#39;) lines(xx$speed, predict(cars_xfrm, xx, interval=&quot;confidence&quot;)[, 3]^2, lty=2, col=&#39;red&#39;) ## add lines to show a prediction interval for new data points lines(xx$speed, predict(cars_xfrm, xx, interval=&quot;prediction&quot;)[, 2]^2, lty=3, col=&#39;darkorange&#39;) lines(xx$speed, predict(cars_xfrm, xx, interval=&quot;prediction&quot;)[, 3]^2, lty=3, col=&#39;darkorange&#39;) The happy result is a model where the mean predictions are about the same as before, but the predictive interval is a better match to the data. 5.2.2 Breast cancer example We used logistic regression to create a model for five year survival among women in the German Breast Cancer Study Group. To validate the assumption of a binomial distribution with a logistic link, we will bin predictions into groups that have similar predicted probability of survival, and then check that the observed frequency of survival in each bin follows the assumed binomial distribution. # Calculate likelihoods of the observed five-year survival bins = sapply( 1:20, function(k) pbinom( fys_gbsg[[k]], tot_gbsg[[k]], p_gbsg[[k]] ) ) # histogram of the bin likelihoods hist( bins, breaks=10 ) The histogram shows that the likelihoods of the observed data are clustered near the extremes (0 and 1). If the observations match the assumed binomial distribution, then we would expect likelihoods to be uniformly distributed between zero and one (no clustering). So this model for the five year survival probability should not pass validation - it needs more work. 5.2.3 COVID admissions example Since each day’s prediction is assumed to come from a Poisson distribution, that assumption defines an expected range for the observed data. The qpois(), dpois(), ppois() family of functions is used for this purpose. Here, we will validate the assumed Poisson distribution by calculating the frequency that the observed data is within the predicted 80% confidence interval. If the nominal coverage is correct, the frequency should be about 80%. # "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
