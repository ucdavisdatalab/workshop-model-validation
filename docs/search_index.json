[["index.html", "Model validation for applied data science Overview", " Model validation for applied data science Wesley Brooks 2021-11-18 Overview YOUR DESCRIPTION, LEARNING GOALS, PREREQUISITES, ETC "],["beginnings.html", "1 Beginnings 1.1 Why model validation? 1.2 What is model validation?", " 1 Beginnings Validating a model is the process of testing it to decide whether the assumptions that are built into the model’s construction are valid. Typically, any kind of statistical method entails some assumptions about the data. That usually means assumptions about the population from which the data were sampled. It is generally impossible to confirm an assumption like, “samples from the population are independent” or, “the population is normally distributed”. Instead, we follow the scientific method of proposing some phenomenon that is bound to happen if the assumption is true, and then if that phenomenon does not occur we have evidence against the assumption. Statistical modeling and model validation begin no later than your first look at your data. This is when you can begin to ask questions like, - “Are there limits to the range of the response variable?” - “What is the distribution of the response variable?” - “Based on my prior knowledge, am I able to anticipate the structure of the model?” If you have answers to these, then you are well on your way to creating a great model. With that in mind, I’m going to start this workshop by introducing examples of data. 1.1 Why model validation? You can do a lot with exploratory data analysis and descriptive statistics. But not everything! There are some very common scientific tasks that require not just description of the data, but statistical models that use the observed data to illuminate some hidden truth about the world. For instance, you may want to know about the relationship between some variables or you may want to predict what will be the value of the next sample. If the conditions are right, we can answer those kinds of questions with a model. What does it mean for the conditions to be right? It means that the model is suited to the task and to the data. Confirming these is the tasks of model validation. 1.2 What is model validation? Statistical models rely on assumptions, and model validation means testing those assumptions. There are three general assumptions that underlie most statistical models: Observations are usually assumed to be independent of each other (or assume some specific form of dependence). The expectation of the response variable is a function of the input variables. \\(\\mathbb{E}(Y) = f(X)\\). The response variable has some distribution, like \\(Y\\) is Normal, or binomial, or Poisson. Validating these assumptions also requires that the function \\(f(\\cdot)\\) and the response distribution can be learned from the data. So, how can we test (validate) these assumptions? A key problem is that they generally can’t be tested from within a fitted model. The way statistical estimation works is that you assume the form of a model, and then find the parameters that best match the data and the assumption. There is a circularity to the logic of estimating a model that requires some assumption about the data and then using that same data to check whether the assumptions are violated. Testing requires applying the model to new data in order to see if the predictions match the truth. This is a two-part process, and data science involves a lot of trial and error. On the way to creating a well-functioning model, you’ll typically do a lot of exploratory analysis. You’ll follow dead ends, try out ideas, make mistakes, and have to start all over when your collaborators completely renovate the data set. It is only natural to learn to prefer the process that performs best over this process. But as the proof of the pudding is in the eating, the real value of your modeling effort is what happens after it leaves your office and goes out into the big world, where it won’t have a kind and caring data scientist to hold its hand. Thus, the second part of the model-making process is the validation portion, and it is extremely important that you have some held-out validation data that can be used as a test of what will happen when your model encounters data that it hasn’t been coached on. "],["example-covid-19-hospital-admissions.html", "2 Example: COVID-19 hospital admissions", " 2 Example: COVID-19 hospital admissions Let’s load the data for the examples that we’ll be working through today. First, though, we need to load some packages and set the path to use for loading data. library( &quot;dplyr&quot; ) library( &quot;lubridate&quot; ) library( &quot;readr&quot; ) library( &quot;gbm&quot; ) # set the root path for loading data. data_path = &quot;https://raw.githubusercontent.com/ucdavisdatalab/workshop-model-validation/master/data&quot; The data sets that we’ll use for illustration are a time series of daily hospital admissions for COVID-19, and a time series of the number of beds occupied overnight by COVID-19 patients. Our goal will be to create models that can predict the future values of these time series. I like to begin with a pair plot of all the data, but here we have 81 columns, which is too many to fit on a pair plot. Instead, we’ll look at the time series. # import the covid dataset covid = read_csv( file.path(data_path, &quot;covid.csv&quot;) ) # plot the admissions time series with( covid, plot(date, COVID_NEW_ADM_CNT, type=&#39;l&#39;) ) The count of daily admissions is a nonnegative integer. It appears that there is a lot more variability in the admissions count when the recent rate of admissions is near its peaks than when it is near zero. There isn’t a theoretical maximum for daily admissions (though of course there would be a practical maximum, if the hospital became overwhelmed). And we can reasonably assume that each admission was an independent decision. To a statistician, those facts all point toward trying to assume that the counts follow a Poisson distribution. There is another type of independence to consider here: is the number of daily admissions independent from the number of admissions the day before? That kind of independence doesn’t mean you pretend that there’s no information in the most recent counts - instead, it means that the short-term fluctuations are as likely to be above the trend as below it. Without that independence, we would say that the data are autocorrelated. And we can test for autocorrelation: # plot the autocorelation function of the diff-1 covid admissions acf( diff(covid$COVID_NEW_ADM_CNT, 1) ) You can ignore the first line of the chart - it is there to show how long a line is drawn to show perfect correlation at zero time lag. The salient feature of this plot is that the third line points downward, and crosses the dashed blue line. There is apparently a tendency for the count to decrease the day after it increases, and vice versa. This may or may not be due to some trend in the covariates - we’ll check again after accounting for the explanatory variables. "],["model-types.html", "3 Model types 3.1 Linear regression 3.2 Boosted forest regression", " 3 Model types The data examples each require a different kind of model, which will allow us to demonstrate several different instances of model validation. 3.1 Linear regression We will use linear regression to create a model that relates a car’s speed to the distance required for stopping the car. Linear regression can use transformed versions of the inputs and outputs, and we will test various combinations of transformations to decide which one is the best. The functional form of a linear regression model looks like \\[ y = X \\beta + \\varepsilon \\] where \\(X\\) are the (possibly transformed) covariates, \\(\\beta\\) are the regression coefficients, \\(\\varepsilon\\) are residual errors, and \\(y\\) is the (possibly transformed) response variable. Linear regression assumes that the observations are independent, that the functional form accurately describes the relationship between inputs and response, and that the residual errors are follow a normal distribution with mean zero and constant variance. 3.2 Boosted forest regression Random forest regression models are an entirely different class of model. There is no linear function like the \\(X \\beta\\) of logistic regression (or, indeed, linear regression). Instead, the \\(f( X )\\) part of the model is constructed from an ensemble of regression trees. Each regression tree is a collection of discrete branches that ultimately lead to conclusions at the end of the chain. Consider the stopping distance example again. One regression tree might include logic like, IF speed &gt; 8 AND speed &lt;= 12 THEN stoping distance is 20 feet. As a forest is made of many trees, a random forest model is made of many regression trees, each of which uses a random portion of the total training data. In fact, the GBM models I’ll use here include some additional “gradient boosting” logic that takes them beyond a basic random forest model, but that I’m going to mostly ignore for our purposes today. The response variable for the random forest model is the number of new COVID-19 admissions on a given day. I’m treating this count as if it follows a Poisson distribution. Thus, the assumptions that must be validated for this model are the relationship \\(\\mathbb{E}(Y) = f( X )\\), and the independence and Poisson distribution of the counts. "],["data-splitting-systems.html", "4 Data splitting systems 4.1 Train/validation split 4.2 Cross-validation", " 4 Data splitting systems To train and validate a model requires that we have in-sample and out-of-sample data, but typically we have just “the data”. There are a couple of approaches to separating that data in in-sample and out-of-sample sets: training/validation splits or cross-validation. Theyare often used together. 4.1 Train/validation split One solution is to reserve some data for validation, and use what is left for training the model. The split can be random or not - for instance, you may hold back the most recent year of data for validation, or you may randomly sample some proportion (e.g., 30%) of the observations to reserve for validation. 4.2 Cross-validation Cross validation is a kind of repeated training/validation split, which is iterated until all of the data has been used for testing. Each training/validation split may be random or may take data that are grouped according to some meaningful value. For instance, time-series data may be best analyzed by holding out contiguous blocks of observations. Here, I’ll demonstrate doing three-fold cross-validation on the car stopping distance data. I’ll split the data into three folds and use those divisions to estimate three models. Each model uses two folds as training data and one as validation data. Some things to note are that the estimated line of best fits are different for each iteration of the CV, and that the predictions (in the right hand panels) are generally less accurate than the fitted values (in the left hand panels). 4.2.1 Time series data When the data are a time series, it is good practice for validation splits to respect the time ordering of the observations. That’s because the results of validation are more realistic when they work from a known past to an unknown future. That means setting a split date, then training over past data and predicting future observations. You can still use cross-validation for time-series data - it is called walkforward validation, where each fold is a contiguous temporal block, and only past blocks are used in predicting the next block in the sequence. 4.2.2 Combining the two There are times when cross-validation and training/testing validation should be used together. For instance, when cross-validation is used for exploratory analysis and model selection, then validation should be done using new data that was never previously part of the estimation. "],["example-overnight-bed-occupancy.html", "5 Example: Overnight bed occupancy 5.1 Checking whether observations are independent 5.2 Validating the distribution for a linear model 5.3 Filter candidates by cross-validation 5.4 Final validation / selection", " 5 Example: Overnight bed occupancy Recall that one of our goals is to predict the number of hospital beds that will be occupied tonight by COVID-19 patients. Before beginning, let’s split the data in half - we’ll use the first half for model building and intermediate validation, and reserve the second half for final validation. # add a variable to covid for the one-day-ahead census (the prediction target) covid$D1_census = lead( covid$COVID_PT_CNT_M, 1 ) # create a 50/50 train/validation split of the covid data covid_split = validation_time_split( covid, prop=0.5 ) # extract the covid_train data set covid_train = training( covid_split$splits[[1]] ) 5.1 Checking whether observations are independent That’s count data, but we may suspect that the nightly counts are not independent, because it is common for a person to spend consecutive nights in the hospital. Let’s check the autocorrelation of the counts to see if our hunch is correct. layout( matrix(1:2, 1, 2) ) with( covid_train, plot( date, COVID_PT_CNT_M, type=&#39;l&#39;) ) acf( covid_train$COVID_PT_CNT_M ) The plot on the left is the time series of overnight census counts, and the plot on the right is the autocorrelation function. The gist of the autocorrelation function is that the “Lag” indicates a number of days separating data points, and when the lines are far above or below zero then there is correlation between points that are separated by that many days. The correlation at lag zero is always one because that is comparing a day’s data to itself (separated by zero days). There is a lot of autocorrelation in this time series. This may be ok if we can “control for” the correlation. It is possible to control for autocorrelation if the long-term correlation between distant observations is the same as the relationship between neighboring observations, iterated for every data point that separates the two distant observations. In that case, the data are independent, conditional on the previous day’s count. In practice that means including the previous day’s count as a predictor in the model. The way to think about The smoothly decaying autocorrelation as the lag time increases tells us that the nightly counts are correlated with the previous count in a consistent pattern. In that case, the increments may be independent, even though the individual observations are not. layout( matrix(1:2, 1, 2) ) with( covid_train, plot( date, c(NA, diff(COVID_PT_CNT_M, 1 )), type=&#39;l&#39;)) acf( diff(covid_train$COVID_PT_CNT_M, 1) ) Now the autocorrelation looks much better because most of the vertical lines are very small (the first line will always touch a maximum value of 1 by definition). This is an indication that a linear model for the nightly bed occupancy should use the previous night’s occupancy as a predictor. 5.2 Validating the distribution for a linear model A linear model (like what you get from R’s lm() function) comes with some assumptions, and helpfully provides simple tools to help you validate them. Among these are assumptions that the errors have a normal distribution with constant variance. Let’s look at how to test that. 5.2.1 Examiune diagnostic plots Validating the assumptions about the distribution of errors in a linear model is usually done through the diagnostic plots. These are four plots you get when you plot() a fitted lm() model. We already know that our example will use the previous night’s occupancy as a predictor, so let’s start with the simplest model of that kind. # estimate the AR1 model census_model = lm( D1_census ~ COVID_PT_CNT_M, data=covid_train ) # show the diagnostic plots in a 2x2 layout layout( matrix( 1:4, 2, 2 ) ) par(mar=c(4,4,2,1)) plot( census_model ) These plots reveal some classic problems with the linear model census_model. Learn to recognize these, and you’re well on your way to doing good data analysis. We will look at the first three, since the fourth plot is less often relevant, and requires a deeper understanding of the model’s structure. 5.2.1.1 Residual vs. fitted The ideal is for the points in this plot to exhibit no pattern. But the points in this plot are arrayed in a fan shape that widens to the right. This suggests that the residuals get more variable as the fitted value increases. The other common problem that could be seen in the residual vs. fitted plot is a “U” shape (or upside-down “U”), which would suggest that the relationship between the predictors and the response is not linear. We don’t have any kind of “U” shape here, though. 5.2.1.2 Scale-location The ideal for this plot is also to have no apparent pattern. But in this example, we can see that the points are more tightly clustered on the left, and more spread out on the right. As in the Residual vs. Fitted plot, this is an indication that the residuals are not constant for different fitted values. The red line is a smoothing line that helps clarify the pattern in the dots. 5.2.1.3 Normal Q-Q This plot helps you to validate the assumption that the residuals are from a normal distribution with constant variance. The ideal for this plot is to have all of the dots lie on the dotted line. That’s not the case here, as the dots at both ends bend away from the dotted line. This is an indication of “heavy tails”, which may be because the residuals are more variable as the fitted values increase. 5.2.2 What to do about the diagnostic plots The patterns seen here suggest that it may be appropriate to transform the response variable in a way that compresses the larger values, relative to the smaller values. The most commonly used transformations of this kind are a logarithm or a square root transformation. There is no “U” shape in the residual vs. fitted plot, so we see no problem with the assumption that the response is a linear function of the predictors. Therefore, in order to not spoil the linear relationship, we should apply the same transformation to the predictor as to the response. 5.2.3 Try a log transform Here’s the code to fit a model with the log transform and generate the diagnostic plots. # estimate the log model census_model_log = lm( log(D1_census) ~ log(COVID_PT_CNT_M), data=covid_train ) # show the diagnostic plots in a 2x2 layout layout( matrix( 1:4, 2, 2 ) ) par(mar=c(4,4,2,1)) plot( census_model_log ) The patterns in the Residual vs. Fitted and Scale-Location plots are the opposite of what they were for the untransformed model (the fan opens to the left, and the scale gets smaller for greater fitted values). Once again, it looks like the assumption of normal residuals with constant variance isn’t supported. 5.2.4 Try square root transform The log transform went too far in compressing the greater counts relative to the smaller counts. The square root transforms data in a similar way, qualitatively, but is less extreme. Let’s see what happens when we use that transform instead. # estimate the square-root model census_model_sqrt = lm( sqrt(D1_census) ~ sqrt(COVID_PT_CNT_M), data=covid_train ) # show the diagnostic plots in a 2x2 layout layout( matrix( 1:4, 2, 2 ) ) par(mar=c(4,4,2,1)) plot( census_model_sqrt ) Now, these diagnostic plots are not exactly perfect, but they are about as close as you can get with real-world data. Under the square root transformation, we have validated the assumption that the model residuals are normally distributed with constant variance. 5.3 Filter candidates by cross-validation The final step in this example is to demonstrate selecting the variables for the linear regression model. Since the model will be used in prediction, we want to pick the set of predictors that generate the best performance on out-of-sample data. To do so, we need to test the candidates on out-of-sample data, and pick the one that performs best. We have reserved some out-of sample data, but that is for final validation. While we sort through candidate models, let us continue to use the training data. We will use cross-validation to simulate how it does over new data. Once we have a few candidate models in mind, we will use the validation data to pick one of them. # set a random seed for the sake of reproducibility set.seed(20211119) # Create a list of five candidate models formulas = list( m1 = sqrt(D1_census) ~ sqrt(COVID_PT_CNT_M), m2 = sqrt(D1_census) ~ sqrt(COVID_PT_CNT_M) + COVID_NEW_ADM_MEAN, m3 = sqrt(D1_census) ~ sqrt(COVID_PT_CNT_M) + COVID_NEW_ADM_MEAN + POSITIVITY_RATE_MEAN, m4 = sqrt(D1_census) ~ sqrt(COVID_PT_CNT_M) + COVID_NEW_ADM_MEAN + POSITIVITY_RATE_MEAN + COVID_DISCHARGE_MEAN, m5 = sqrt(D1_census) ~ sqrt(COVID_PT_CNT_M) + COVID_NEW_ADM_MEAN + POSITIVITY_RATE_MEAN + COVID_DISCHARGE_MEAN + COVID_PT_CNT_MIDNIGHT_MEAN ) # create ten cross-validation folds covid_cv = vfold_cv( covid_train, v=10 ) # create a workflow set of candidate models candidates = workflow_set(preproc = formulas, models = list(lm = linear_reg())) # run the candidates on the CV folds cv_result = workflow_map( candidates, &quot;fit_resamples&quot;, resamples = covid_cv) # view the ranked results rank_results( cv_result, rank_metric=&quot;rmse&quot; ) %&gt;% filter(.metric == &#39;rmse&#39;) ## # A tibble: 5 × 9 ## wflow_id .config .metric mean std_err n preprocessor model rank ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 m5_lm Preprocessor1… rmse 0.197 0.00919 10 formula linear… 1 ## 2 m3_lm Preprocessor1… rmse 0.197 0.00963 10 formula linear… 2 ## 3 m4_lm Preprocessor1… rmse 0.198 0.00995 10 formula linear… 3 ## 4 m2_lm Preprocessor1… rmse 0.203 0.0103 10 formula linear… 4 ## 5 m1_lm Preprocessor1… rmse 0.206 0.00971 10 formula linear… 5 5.4 Final validation / selection Based on the exploratory modeling work, we conclude that models 5, 3, and 4 are the best candidates. Then we’ll send these three to the final validation stage, where we select the one that has the best accuracy over the validation data that was held out from the exploratory and modeling work. # validation candidates are models 5, 4, and 3 valcan = candidates[c(5,3,4), ] # run the validation candidates models on the validation data val_result = workflow_map( valcan, &quot;fit_resamples&quot;, resamples = covid_split) # view the ranked results rank_results( val_result, rank_metric=&quot;rmse&quot; ) %&gt;% filter(.metric == &#39;rmse&#39;) ## # A tibble: 3 × 9 ## wflow_id .config .metric mean std_err n preprocessor model rank ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 m3_lm Preprocessor1… rmse 0.215 NA 1 formula linear… 1 ## 2 m4_lm Preprocessor1… rmse 0.216 NA 1 formula linear… 2 ## 3 m5_lm Preprocessor1… rmse 0.217 NA 1 formula linear… 3 So we would select model three as the one that performs best in prediction. Here’s how it looks in prediction: # extract the test data covid_test = testing( covid_split$splits[[1]] ) # create the final model census_model = lm( sqrt(D1_census) ~ sqrt(COVID_PT_CNT_M) + COVID_NEW_ADM_MEAN + POSITIVITY_RATE_MEAN, data=covid_train ) # plot the census test output with( covid_test, plot(date, D1_census) ) # add the lines of the predicted census count lines( covid_test$date, predict( census_model, covid_test )^2, col=&#39;red&#39;, lwd=2) "],["example-covid-19-admissions.html", "6 Example: COVID-19 admissions 6.1 Checking whether observations are independent 6.2 Distribution of the response 6.3 Boosted forest model 6.4 Validating the model", " 6 Example: COVID-19 admissions We turn now to our other example: predicting the number of hospital admissions for COVID-19 in the next day. # create a variable for the one-day ahead admissions: covid$D1_admissions = lead(covid$COVID_NEW_ADM_CNT, 1) # create the validation split covid_split = validation_time_split( covid, prop=0.5 ) # extract the training data admits_train = training( covid_split$splits[[1]] ) 6.1 Checking whether observations are independent Let’s again begin by validating the assumption that the observations are independent. In particular, this is another time series and the most common form of dependency in time series data is for an observation to be correlated with the observations that come before and after. Let’s check this data’s autocorrelation function: layout( matrix(1:2, 1, 2) ) with( admits_train, plot( date, COVID_NEW_ADM_CNT, type=&#39;l&#39;) ) acf( admits_train$COVID_NEW_ADM_CNT ) As in the case of overnight occupancy, there is a lot of autocorrelation. We therefore apply the same technique of checking whether the daily increments are correlated. layout( matrix(1:2, 1, 2) ) with( admits_train, plot( date, c(NA, diff(COVID_NEW_ADM_CNT, 1)), type=&#39;l&#39;) ) acf( diff(admits_train$COVID_NEW_ADM_CNT, 1) ) Here the result is not as clean as it was for the overnight census count, but the imprevement is enough that we can proceed with modeling. 6.2 Distribution of the response Take note, again, of the time series of daily admissions: # plot the daily admissions time series with( admits_train, plot( date, COVID_NEW_ADM_CNT, type=&#39;l&#39;) ) Some salient features: the data are integer counts, non negative. And if you imagine a smooth curve running through the noisy data, you’ll see that when this imagined line is at its peaks, there is a lot of noise in the observations. On the other hand, when the smoothed curve is at its lowest, then the amount of noise in the counts is also at its lowest. These are all features of data that match the Poisson distribution. The other requirement is that the patients are admitted to the hospital independently of each other (we can’t validate this assumption from the data). We will proceed under the assumption that the data are from a Poisson distribution. 6.3 Boosted forest model A boosted forest model is a very flexible form of machine learning model that uses an ensemble of trees (hence the forest term) for predicting the outcome. An individual tree is a collection of binary decisions like, “If there were five people admitted to the hospital for COVID yesterday, and if the rate of positive COVID tests is less than the day before, then predict four admissions today”. A boosted forest often has thousands of trees at a minimum. The main advantage of boosted forest models is that they are very flexible - meaning that can adapt to nonlinear relationships without difficulty. That flexibility means that there are relatively few assumptions for this kind of model. We need the data to be (conditionally) independent, and have validated that. We also assume that the modeled relationship between the predictors and the response is accurate, and we assume that the distribution of the response is as specified in the model. For our example the assumed response distribution is Poisson. 6.4 Validating the model In order to validate the last two assumptions, we will use the same test/validation split as in the prior example. # remove columns that aren&#39;t used in prediction admits = admits_train[, c(2:28, 30:67)] # add the D1_admissions column to admits admits$D1_admissions = admits_train$D1_admissions # set a random seed so that the cross-validation in gbm_wrapper is reproducible set.seed(20211118) # make a call to gbmwrap boost = gbm_wrapper(target=&quot;D1_admissions&quot;, data=admits, distribution=&quot;poisson&quot;) Now that we’ve estimated a model for the data, we can validate the assumption that the model accurately describes the relationship between the predictors an the response, and the assumption that the responses are from a Poisson distribution with the model giving the mean. I’ll demonstrate two approaches to validating the first assumption: first, I’ll compare the error from the model’s predictions to the error you’d see by simply projecting the last data point forward by one day. Then I’ll plot the predicted admission counts against the observations to see if the predictions look reasonably accurate. I’ll validate the assumption that the responses are from a Poisson distribution by adding the confidence intervals of a Poisson distribution to the plot to judge whether they cover the desired proportion of the data. # extract the validation set from the split admits_test = testing( covid_split$splits[[1]] ) # predict counts on the validaton data admits_preds = predict( boost, admits_test, type=&#39;response&#39; ) ## Using 1302 trees... # calculate the mean absolute error of the predictions cat( &quot;Mean absolute error for the model: &quot;, mean( abs( admits_preds - admits_test$D1_admissions ), na.rm=TRUE), &quot;\\nMean absolute error for the lag-1 observations: &quot;, with( admits_test, mean( abs( COVID_NEW_ADM_CNT - D1_admissions), na.rm=TRUE)) ) ## Mean absolute error for the model: 1.644679 ## Mean absolute error for the lag-1 observations: 2.081395 # calculate poisson predictive intervals admits_ci = data.frame( lower=qpois(0.25, admits_preds), upper=qpois(0.75, admits_preds)) # plot the daily admissions in the validation data with(admits_test, plot( date, D1_admissions )) #add a line for the predictions out of the model lines( admits_test$date, admits_preds, col=&#39;red&#39;) #add lines for the prediction 50% interval lines( admits_test$date, admits_ci$lower, col=&#39;blue&#39;) lines( admits_test$date, admits_ci$upper, col=&#39;blue&#39;) # calculate coverage of the prediction interval cat( &quot;Prediction interval coverage (nominal 50%): &quot;, mean(admits_test$D1_admissions &gt;= admits_ci$lower &amp; admits_test$D1_admissions &lt;= admits_ci$upper, na.rm=TRUE )) ## Prediction interval coverage (nominal 50%): 0.5852713 Our boosted forest model has the lower prediction error, and the coverage of the prediction interval is fairly similar to the nominal 50%. This model isn’t perfect, but validation doesn’t reveal any terrible behavior and the model is useful. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
