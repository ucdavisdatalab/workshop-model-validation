[["index.html", "Model validation for applied data science Overview 0.1 Learning goals 0.2 Prerequisites 0.3 Software", " Model validation for applied data science Wesley Brooks 2021-11-19 Overview Welcome to DataLab’s model validation workshop! This reader provides you with a guided tour of the why and how of validating statistical models. Code for all of the examples is provided, and you are encouraged to open up RStudio and code along with the presenter. 0.1 Learning goals In this workshop you will learn: Why model validation is necessary Why you typically validate a model with held-out data What are validation and cross-validation How to split a dataset into training and testing components How to test the assumptions of a linear regression model How to test the assumptions of a boosted forest model 0.2 Prerequisites This workshop will be conducted by working live through examples of validating statistical models in R. It therefore is necessary that you have some experience with R (so that you can follow the examples), and some exposure to statistical models. That said, the workshop is a guided tour, so you should be able to follow along simply by typing the same commands as the instructor, or copy-pasting code from this reader into your RStudio console. As always, you’ll get more from the experience if you’re better prepared. 0.3 Software In order to follow along, you’ll need to have installed R and RStudio. Here are links to those if you need them: Download R Download RStudio Then you’ll need to have installed the R packages that we’ll be using in the workshop. It is very easy to install packages from within R, using the install.packages() function. There is also one package to install from Github (it contains a wrapper function that your humble instructor wrote to make fitting a boosted forest model simpler). Here is the list of commands to install the necessary packages: install.packages( &quot;dplyr&quot; ) install.packages( &quot;readr&quot; ) install.packages( &quot;lubridate&quot; ) install.packages( &quot;tidymodels&quot; ) install.packages( &quot;gbm&quot; ) remotes::install_github( &quot;wes-brooks/gbmwrap&quot; ) "],["beginnings.html", "1 Beginnings 1.1 Why model validation? 1.2 What is model validation?", " 1 Beginnings Validating a model is the process of testing it to decide whether the assumptions that are built into the model’s construction are valid. Typically, any kind of statistical method entails some assumptions about the data. That usually means assumptions about the population from which the data were sampled. It is generally impossible to confirm an assumption like, “samples from the population are independent” or, “the population is normally distributed”. Instead, we validate a model by testing those assumptions - seeing if we can prove them wrong. 1.1 Why model validation? You can do a lot with exploratory data analysis and descriptive statistics. But not everything! There are some very common scientific tasks that require not just description of the data, but statistical models that use the observed data to illuminate some hidden truth about the world. For instance, you may want to know how some predictor is related to a response, or you may want to predict what will be the value of the next sample. If the conditions are right, we can answer those kinds of questions with a model. What does it mean for the conditions to be right? It means that the model is suited to the task and to the data. Confirming these is the tasks of model validation. Of course, a model also has to be trained, and training covers a lot of the same ground as validation. But validation is still necessary because it offers evidence that the training will apply beyond the training data. Data science involves a lot of trial and error. while training a model, you’ll typically do a lot of exploratory analysis. You’ll follow dead ends, try out ideas, make mistakes, or have to start all over when your collaborators completely renovate the data set. It is only natural to learn to prefer the process that performs best over this process. But as the proof of the pudding is in the eating, the real value of your modeling effort is what happens after it leaves your office and goes out into the big world, where it won’t have a kind and caring data scientist to hold its hand. Thus, the second part of the model-making process is the validation portion, and it is extremely important that you have some held-out validation data that can be used as a test of what will happen when your model encounters data that it hasn’t been coached on. 1.2 What is model validation? Statistical models rely on assumptions, and model validation means testing those assumptions. There are three general assumptions that underlie most statistical models: Observations are usually assumed to be independent of each other. The expectation of the response variable is a function of the predictor variables. \\(\\mathbb{E}(Y) = f(X)\\). The response variable has some distribution. For instance, \\(Y\\) is Normal, or binomial, or Poisson. So, how can we validate these assumptions? A key problem is that they generally can’t be tested from within a fitted model. Model training requires making assumptions of the data, so it would be circular logic to then use the same training data to test the assumptions. Validation is the process of testing whether the assumptions are violated when the model is applied to new data. That means making predictions and confirming that they don’t reveal violations of the assumptions. "],["example-data-covid-19-counts.html", "2 Example data: COVID-19 counts", " 2 Example data: COVID-19 counts Before getting into how to split the data into training and validation parts, let’s get to know the examples that we’ll be working through today. First, though, we need to load some packages and set the path to use for loading data. # import libraries library( &quot;dplyr&quot; ) library( &quot;lubridate&quot; ) library( &quot;readr&quot; ) library( &quot;gbm&quot; ) library( &quot;tidymodels&quot;) # import the covid data set covid = read_csv( &quot;https://raw.githubusercontent.com/ucdavisdatalab/workshop-model-validation/master/data/covid.csv&quot; ) The data sets that we’ll use for illustration are a time series of daily hospital admissions for COVID-19, and a time series of the number of beds occupied overnight by COVID-19 patients. Our goal is to create models that can predict the future values of these time series. Let’s take a look at the time series. In the code that follows, you’ll see the with() function, which tells R where to check first for data when it encounters a variable name. That makes the code cleaner to read and write, as there are fewer repetitions of the prefix covid$ to access data. # layout the plot window for side-by-side figures layout( matrix(1:2, 1, 2)) # plot the admissions time series with( covid, plot(date, COVID_NEW_ADM_CNT, type=&#39;l&#39;) ) # plot the overnight census time series with( covid, plot(date, COVID_PT_CNT_M, type=&#39;l&#39;) ) That’s the daily number of new admissions on the left (the variable comes out of the hospital’s records system as COVID_NEW_ADM_CNT), and the number of beds occupied overnight by COVID-19 patients on the right (this variable is coded in the hospital records as COVID_PT_CNT_M, and the hospital refers to the overnight count as the midnight census). The two follow a similar pattern, with cases arriving in waves that rise and fall quickly against a baseline of low counts. There is more noise in the time series of new admissions, while the midnight census is more stable from one day to the next. "],["data-splitting-systems.html", "3 Data splitting systems 3.1 Time series data 3.2 Train/testing split 3.3 Cross-validation 3.4 Combinations", " 3 Data splitting systems To train and validate a model requires that we have in-sample and out-of-sample data, but typically we have just “the data”. There are a couple of approaches to separating that data in in-sample and out-of-sample sets: training/validation split or cross-validation. They are often used together. 3.1 Time series data Our example data are time series, so it is good practice for validation splits to respect the time ordering of the observations. That’s because the results of validation are more realistic when they work from a known past to an unknown future. 3.2 Train/testing split One solution is to reserve some data for validation, and use what is left for training the model. The split can be random or not - for instance, you may hold back the most recent year of data for validation, or you may randomly sample some proportion (e.g., 50%) of the observations to reserve for validation. The rsample package provides a functions called validation_split() and validation_time_split() to split the data into training and testing sets. The difference between the two is that validation_split() does a random split and validation_time_split() does its splitting by keeping the first part of the data for training and the latter part for testing. The same package provides the functions testing() and training() to extract the testing set and the training set, respectively, from the split. For the examples, we will train using the first 50% of the observations and validate using the last 50%. # do a time split on the covid data covid_split = validation_time_split( covid, prop=0.5 ) # inspect the split object covid_split ## # Validation Set Split (0.5/0.5) ## # A tibble: 1 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [259/259]&gt; validation # extract the training set from the split covid_train = training( covid_split$splits[[1]] ) # inspect the training data covid_train ## # A tibble: 259 × 67 ## date DAY_OF_THE_WEEK HOSPITAL_CENSUS… INDX_UCDH_TEST_… INDX_UCDH_POS_P… ## &lt;date&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2020-06-01 1 547 328 1 ## 2 2020-06-02 2 529 290 3 ## 3 2020-06-03 3 529 236 3 ## 4 2020-06-04 4 554 207 1 ## 5 2020-06-05 5 547 317 4 ## 6 2020-06-06 6 521 144 0 ## 7 2020-06-07 0 519 167 1 ## 8 2020-06-08 1 532 364 1 ## 9 2020-06-09 2 540 290 0 ## 10 2020-06-10 3 541 267 3 ## # … with 249 more rows, and 62 more variables: INDX_UCDH_POSITIVITY_RATE &lt;dbl&gt;, ## # INDX_POS_PT_NEW_ADM_CNT &lt;dbl&gt;, INDX_POS_PT_NEW_ADM_PCT &lt;dbl&gt;, ## # INDX_POS_PT_IN_HOUSE_D6_CNT &lt;dbl&gt;, INDX_POS_PT_IN_HOUSE_D6_PCT &lt;dbl&gt;, ## # INDX_POS_PT_IN_HOUSE_D7_CNT &lt;dbl&gt;, INDX_POS_PT_IN_HOUSE_D7_PCT &lt;dbl&gt;, ## # INDX_POS_PT_ALL_ADM_CNT &lt;dbl&gt;, INDX_POS_PT_ALL_ADM_PCT &lt;dbl&gt;, ## # OUTREACH_TEST_CNT &lt;dbl&gt;, OUTREACH_POS_TEST_CNT &lt;dbl&gt;, ## # OUTREACH_POSITIVITY_RATE &lt;dbl&gt;, COVID_RULE_OUT_PT_CNT_M &lt;dbl&gt;, … 3.3 Cross-validation Cross validation (often abbreviated CV) is a kind of repeated training/validation split. The data is broken into several chunks (called “folds”), and one is held out for validation. All of the others are used as a training set, then resulting model is used to predict the response over the held-out fold. The process is iterated until each fold has been held out once. The main benefit of cross validation is that by iterating over the folds, you end up with a prediction for every data point. This can be important when doing a single train/test split would leave too few observations in the test set to draw reliable conclusions for validation. Each training/validation split may be random or may take data that are grouped according to some meaningful value. For instance, time-series data may be best analyzed by holding out contiguous blocks of observations. We will use cross-validation on the training set to help build the models, before we validate them using the test set. As before, the rsample package provides convenient functions to create cross-validation splits that play nicely with other parts of the tidymodels system. Here, the CV folds aren’t using contiguous time blocks, which is a shortcoming. We’re doing it this way because the tidymodels tools for creating and using CV folds don’t provide that functionality. To try to write the loops that would do the job properly is beyond the scope of this workshop. # create ten cross-validation folds on the training set covid_cv = vfold_cv( covid_train, v=10 ) # inspect the CV folds covid_cv ## # 10-fold cross-validation ## # A tibble: 10 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [233/26]&gt; Fold01 ## 2 &lt;split [233/26]&gt; Fold02 ## 3 &lt;split [233/26]&gt; Fold03 ## 4 &lt;split [233/26]&gt; Fold04 ## 5 &lt;split [233/26]&gt; Fold05 ## 6 &lt;split [233/26]&gt; Fold06 ## 7 &lt;split [233/26]&gt; Fold07 ## 8 &lt;split [233/26]&gt; Fold08 ## 9 &lt;split [233/26]&gt; Fold09 ## 10 &lt;split [234/25]&gt; Fold10 3.4 Combinations There are times when cross-validation and training/testing validation should be used together. For instance, when cross-validation is used for exploratory analysis and model selection, then validation should be done using new data that was never previously part of the estimation. That’s how we are handling the examples in this workshop. "],["example-overnight-bed-occupancy.html", "4 Example: Overnight bed occupancy 4.1 Linear regression 4.2 Checking whether observations are independent 4.3 Validating the distribution for a linear model 4.4 Filter candidates by cross-validation 4.5 Final validation / selection", " 4 Example: Overnight bed occupancy Recall that one of our goals is to predict the number of hospital beds that will be occupied tonight by COVID-19 patients. Before beginning, let’s split the data in half - we’ll use the first half for model building and intermediate validation, and reserve the second half for final validation. We have aready demonstrated the validation_time_split() function, but have to repeat it here after creating the response variable D1_census. That’s the midnight census count from one day ahead, so we create it using the lead() command in R, which looks ahead in the data. # add a variable to covid for the one-day-ahead census (the prediction target) covid$D1_census = lead( covid$COVID_PT_CNT_M, 1 ) # create a 50/50 train/validation split of the covid data covid_split = validation_time_split( covid, prop=0.5 ) # extract the covid_train data set covid_train = training( covid_split$splits[[1]] ) 4.1 Linear regression This example uses linear regression to create a model for predicting the midnight census. Linear regression works well for that data because the midnight census is very similar to the count from the night prior, and the amount of noise in the time series looks about the same between waves of high counts and the times of low cases. The functional form of a linear regression model looks like \\[ y = X \\beta + \\varepsilon \\] where \\(X\\) are the predictors, \\(\\beta\\) are the regression coefficients, \\(\\varepsilon\\) are residual errors, and \\(y\\) is the response. In our example, the response is the midnight census count, and the predictors are a set of variables that were extracted from the hospital’s database. Both the midnight census and the predictors have one observation per day in the data set. Linear regression assumes that the observations are independent, that the linea form accurately describes the relationship between predictors and response, and that the residual errors are follow a normal distribution with mean zero and constant variance. The rest of this chapter covers testing those assumptions. 4.2 Checking whether observations are independent That’s count data, but we may suspect that the nightly counts are not independent, because it is common for a person to spend consecutive nights in the hospital. Let’s check the autocorrelation of the counts to see if our hunch is correct. layout( matrix(1:2, 1, 2) ) with( covid_train, plot( date, COVID_PT_CNT_M, type=&#39;l&#39;) ) acf( covid_train$COVID_PT_CNT_M ) The plot on the left is the time series of overnight census counts, and the plot on the right is the autocorrelation function. The gist of the autocorrelation function is that the “Lag” indicates a number of days separating data points, and when the lines are far above or below zero then there is correlation between points that are separated by that many days. The correlation at lag zero is always one because that is comparing a day’s data to itself (separated by zero days). There is a lot of autocorrelation in this time series. This may be ok if we can “control for” the correlation. It is possible to control for autocorrelation if the long-term correlation between distant observations is the same as the relationship between neighboring observations, iterated for every data point that separates the two distant observations. In that case, the data are independent, conditional on the previous day’s count. In practice that means including the previous day’s count as a predictor in the model. The way to think about The smoothly decaying autocorrelation as the lag time increases tells us that the nightly counts are correlated with the previous count in a consistent pattern. In that case, the increments may be independent, even though the individual observations are not. layout( matrix(1:2, 1, 2) ) with( covid_train, plot( date, c(NA, diff(COVID_PT_CNT_M, 1 )), type=&#39;l&#39;)) acf( diff(covid_train$COVID_PT_CNT_M, 1) ) Now the autocorrelation looks much better because most of the vertical lines are very small (the first line will always touch a maximum value of 1 by definition). This is an indication that a linear model for the nightly bed occupancy should use the previous night’s occupancy as a predictor. 4.3 Validating the distribution for a linear model A linear model (created by a call to R’s lm() function) comes with some assumptions, and helpfully provides simple tools to help you validate them. Among these are assumptions that the errors have a normal distribution with constant variance. Let’s look at how to test that. 4.3.1 Examiune diagnostic plots Validating the assumptions about the distribution of errors in a linear model is usually done through the diagnostic plots. These are four plots you get when you plot() a fitted lm() model. We already know that our example will use the previous night’s occupancy as a predictor, so let’s start with the simplest model of that kind. # estimate the AR1 model census_model = lm( D1_census ~ COVID_PT_CNT_M, data=covid_train ) # show the diagnostic plots in a 2x2 layout layout( matrix( 1:4, 2, 2 ) ) par(mar=c(4,4,2,1)) plot( census_model ) These plots reveal some classic problems with the linear model census_model. Learn to recognize these, and you’re well on your way to doing good data analysis. The fourth plot is not often relevant, and requires a deeper understanding of the model’s structure. We will go through the first three, one by one. 4.3.1.1 Residual vs. fitted The ideal is for the points in this plot to exhibit no pattern. But the points in this plot are arrayed in a fan shape that widens to the right. This suggests that the residuals get more variable as the fitted value increases. The other common problem that could be seen in the residual vs. fitted plot is a “U” shape (or upside-down “U”), which would suggest that the relationship between the predictors and the response is not linear. We don’t have any kind of “U” shape here, though. 4.3.1.2 Scale-location The ideal for this plot is also to have no apparent pattern. But in this example, we can see that the points are more tightly clustered on the left, and more spread out on the right. As in the Residual vs. Fitted plot, this is an indication that the residuals are not constant for different fitted values. The red line is a smoothing line that helps clarify the pattern in the dots. 4.3.1.3 Normal Q-Q This plot helps you to validate the assumption that the residuals are from a normal distribution with constant variance. The ideal for this plot is to have all of the dots lie on the dotted line. That’s not the case here, as the dots at both ends bend away from the dotted line. This is an indication of “heavy tails”, which may be because the residuals are more variable as the fitted values increase. 4.3.2 What to do about the diagnostic plots The patterns seen here suggest that it may be appropriate to transform the response variable in a way that compresses the larger values, relative to the smaller values. The most commonly used transformations of this kind are a logarithm or a square root transformation. There is no “U” shape in the residual vs. fitted plot, so we see no problem with the assumption that the response is a linear function of the predictors. Therefore, in order to not spoil the linear relationship, we should apply the same transformation to the predictor as to the response. 4.3.3 Try a log transform Here’s the code to fit a model with the log transform and generate the diagnostic plots. # estimate the log model census_model_log = lm( log(D1_census) ~ log(COVID_PT_CNT_M), data=covid_train ) # show the diagnostic plots in a 2x2 layout layout( matrix( 1:4, 2, 2 ) ) par(mar=c(4,4,2,1)) plot( census_model_log ) The patterns in the Residual vs. Fitted and Scale-Location plots are the opposite of what they were for the untransformed model (the fan opens to the left, and the scale gets smaller for greater fitted values). Once again, it looks like the assumption of normal residuals with constant variance isn’t supported. 4.3.4 Try square root transform The log transform went too far in compressing the greater counts relative to the smaller counts. The square root transforms data in a similar way, qualitatively, but is less extreme. Let’s see what happens when we use that transform instead. # estimate the square-root model census_model_sqrt = lm( sqrt(D1_census) ~ sqrt(COVID_PT_CNT_M), data=covid_train ) # show the diagnostic plots in a 2x2 layout layout( matrix( 1:4, 2, 2 ) ) par(mar=c(4,4,2,1)) plot( census_model_sqrt ) Now, these diagnostic plots are not exactly perfect, but they are about as close as you can get with real-world data. Under the square root transformation, we have validated the assumption that the model residuals are normally distributed with constant variance. 4.4 Filter candidates by cross-validation The final step in this example is to demonstrate selecting the variables for the linear regression model. Since the model will be used in prediction, we want to pick the set of predictors that generate the best performance on out-of-sample data. To do so, we need to test the candidates on out-of-sample data, and pick the one that performs best. We have reserved some out-of sample data, but that is for final validation. While we sort through candidate models, let us continue to use the training data. We will use cross-validation to simulate how it does over new data. Once we have a few candidate models in mind, we will use the validation data to pick one of them. This block of code uses some slick functions to simplify the process of running candidates over the cross-validation folds, and compiling the results. The functions workflow_set(), workflow_map(), and rank_results() come from the tidymodels packages. The first one creates a set of models that can be run together, the second evaluates the models of the set, and the third puts the results in order of their predictive performance. # set a random seed for the sake of reproducibility set.seed(20211119) # Create a list of five candidate models formulas = list( m1 = sqrt(D1_census) ~ sqrt(COVID_PT_CNT_M), m2 = sqrt(D1_census) ~ sqrt(COVID_PT_CNT_M) + COVID_NEW_ADM_MEAN, m3 = sqrt(D1_census) ~ sqrt(COVID_PT_CNT_M) + COVID_NEW_ADM_MEAN + POSITIVITY_RATE_MEAN, m4 = sqrt(D1_census) ~ sqrt(COVID_PT_CNT_M) + COVID_NEW_ADM_MEAN + POSITIVITY_RATE_MEAN + COVID_DISCHARGE_MEAN, m5 = sqrt(D1_census) ~ sqrt(COVID_PT_CNT_M) + COVID_NEW_ADM_MEAN + POSITIVITY_RATE_MEAN + COVID_DISCHARGE_MEAN + COVID_PT_CNT_MIDNIGHT_MEAN ) # create ten cross-validation folds covid_cv = vfold_cv( covid_train, v=10 ) # create a workflow set of candidate models candidates = workflow_set(preproc = formulas, models = list(lm = linear_reg())) # run the candidates on the CV folds cv_result = workflow_map( candidates, &quot;fit_resamples&quot;, resamples = covid_cv) # view the ranked results rank_results( cv_result, rank_metric=&quot;rmse&quot; ) %&gt;% filter(.metric == &#39;rmse&#39;) ## # A tibble: 5 × 9 ## wflow_id .config .metric mean std_err n preprocessor model rank ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 m5_lm Preprocessor1… rmse 0.197 0.00919 10 formula linear… 1 ## 2 m3_lm Preprocessor1… rmse 0.197 0.00963 10 formula linear… 2 ## 3 m4_lm Preprocessor1… rmse 0.198 0.00995 10 formula linear… 3 ## 4 m2_lm Preprocessor1… rmse 0.203 0.0103 10 formula linear… 4 ## 5 m1_lm Preprocessor1… rmse 0.206 0.00971 10 formula linear… 5 4.5 Final validation / selection Based on the exploratory modeling work, we conclude that models 5, 3, and 4 are the best candidates. Then we’ll send these three to the final validation stage, where we select the one that has the best accuracy over the validation data that was held out from the exploratory and modeling work. # validation candidates are models 5, 4, and 3 valcan = candidates[c(5,3,4), ] # run the validation candidates models on the validation data val_result = workflow_map( valcan, &quot;fit_resamples&quot;, resamples = covid_split) # view the ranked results rank_results( val_result, rank_metric=&quot;rmse&quot; ) %&gt;% filter(.metric == &#39;rmse&#39;) ## # A tibble: 3 × 9 ## wflow_id .config .metric mean std_err n preprocessor model rank ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 m3_lm Preprocessor1… rmse 0.215 NA 1 formula linear… 1 ## 2 m4_lm Preprocessor1… rmse 0.216 NA 1 formula linear… 2 ## 3 m5_lm Preprocessor1… rmse 0.217 NA 1 formula linear… 3 So we would select model three as the one that performs best in prediction. Here’s how it looks in prediction: # extract the test data covid_test = testing( covid_split$splits[[1]] ) # create the final model census_model = lm( sqrt(D1_census) ~ sqrt(COVID_PT_CNT_M) + COVID_NEW_ADM_MEAN + POSITIVITY_RATE_MEAN, data=covid_train ) # plot the census test output with( covid_test, plot(date, D1_census) ) # predict the census counts over the testing set and square them predicted = predict( census_model, covid_test )^2 # add the lines of the predicted census count lines( covid_test$date, predicted, col=&#39;red&#39;, lwd=2) The mean absolute predictive error can give us an idea of the typical amount of error to expect in future predictions. # calculate the mean absolute predictive error for the census model cat( &quot;MAPE = &quot;, mean(abs( predicted - covid_test$D1_census), na.rm=TRUE)) ## MAPE = 2.168958 "],["example-covid-19-admissions.html", "5 Example: COVID-19 admissions 5.1 Boosted forest model 5.2 Checking whether observations are independent 5.3 Distribution of the response 5.4 Boosted forest model 5.5 Validating the model", " 5 Example: COVID-19 admissions We turn now to our other example: predicting the number of hospital admissions for COVID-19 in the next day. # create a variable for the one-day ahead admissions: covid$D1_admissions = lead(covid$COVID_NEW_ADM_CNT, 1) # create the validation split covid_split = validation_time_split( covid, prop=0.5 ) # extract the training data admits_train = training( covid_split$splits[[1]] ) 5.1 Boosted forest model This example will use a booseted forest model for prediction. A boosted forest model is a very flexible form of machine learning model that uses an ensemble of trees (hence the forest term) for predicting the outcome. An individual tree is a collection of binary decisions like, “If there were five people admitted to the hospital for COVID yesterday, and if the rate of positive COVID tests is less than the day before, then predict four admissions today”. A boosted forest often has thousands of trees at a minimum. The main advantage of boosted forest models is that they are very flexible - meaning that they can adapt to nonlinear relationships without difficulty. That flexibility means that there are relatively few assumptions for this kind of model. Here, we will validate assumptions that the data are independent, that the model’s relationship between the predictors and the response is accurate, and that the distribution of the response is as specified in the model. For our example the assumed response distribution is Poisson. 5.2 Checking whether observations are independent Let’s again begin by validating the assumption that the observations are independent. In particular, this is another time series and the most common form of dependency in time series data is for an observation to be correlated with the observations that come before and after. Let’s check this data’s autocorrelation function: layout( matrix(1:2, 1, 2) ) with( admits_train, plot( date, COVID_NEW_ADM_CNT, type=&#39;l&#39;) ) acf( admits_train$COVID_NEW_ADM_CNT ) As in the case of overnight occupancy, there is a lot of autocorrelation. We therefore apply the same technique of checking whether the daily increments are correlated. layout( matrix(1:2, 1, 2) ) with( admits_train, plot( date, c(NA, diff(COVID_NEW_ADM_CNT, 1)), type=&#39;l&#39;) ) acf( diff(admits_train$COVID_NEW_ADM_CNT, 1) ) Here the result is not as clean as it was for the overnight census count, but the imprevement is enough that we can proceed with modeling. 5.3 Distribution of the response Take note, again, of the time series of daily admissions: # plot the daily admissions time series with( admits_train, plot( date, COVID_NEW_ADM_CNT, type=&#39;l&#39;) ) Some salient features: the data are integer counts, non negative. And if you imagine a smooth curve running through the noisy data, you’ll see that when this imagined line is at its peaks, there is a lot of noise in the observations. On the other hand, when the smoothed curve is at its lowest, then the amount of noise in the counts is also at its lowest. These are all features of data that match a Poisson distribution. The other requirement is that the patients are admitted to the hospital independently of each other (we can’t validate this assumption from the data). We will proceed under the assumption that the data are from a Poisson distribution. 5.4 Boosted forest model Recall that we are planning to use a boosted forest model to predict the number of new admissions. The model type was chosen for its flexibility, but that flexibility can come with complexity as there are several tuning parameters to set for a boosted forest model. I’ve provided a simple wrapper function called gbm_wrapper() in the gbmwrap package, which sets tuning parameters to useful default values, and automates the feature selection process, which we don’t have time to talk about today. There is an internal cross-validation in gbm_wrapper() to optimize the tuning parameters. # remove columns that aren&#39;t used in prediction admits = admits_train[, c(2:28, 30:67)] # add the D1_admissions column to admits admits$D1_admissions = admits_train$D1_admissions # set a random seed so that the cross-validation in gbm_wrapper is reproducible set.seed(20211118) # make a call to gbmwrap boost = gbm_wrapper(target=&quot;D1_admissions&quot;, data=admits, distribution=&quot;poisson&quot;) 5.5 Validating the model For a boosted forest model, we have already validated the assumption that the data are (conditionally) independent. It remains to test that the model’s relationship between the predictors and the response is accurate, and that the distribution of the response is as specified in the model (in this case, Poisson). In order to validate the last two assumptions, we will use the same test/validation split as in the prior example. Now that we’ve estimated a model for the data, we can validate the assumption that the model accurately describes the relationship between the predictors an the response, and the assumption that the responses are from a Poisson distribution with the model giving the mean. I’ll demonstrate two approaches to validating the first assumption: first, I’ll compare the error from the model’s predictions to the error you’d see by simply projecting the last data point forward by one day. I’ll also plot the predicted admission counts against the observations to see if the predictions look reasonably accurate. To validate the assumption that the responses are from a Poisson distribution, add the confidence intervals of a Poisson distribution to the plot and judge whether they cover the desired proportion of the data. # extract the validation set from the split admits_test = testing( covid_split$splits[[1]] ) # predict counts on the validaton data admits_preds = predict( boost, admits_test, type=&#39;response&#39; ) ## Using 1302 trees... # calculate the mean absolute error of the predictions cat( &quot;Mean absolute error for the boosted forest model: &quot;, mean( abs( admits_preds - admits_test$D1_admissions ), na.rm=TRUE), &quot;\\nMean absolute error for the lag-1 observations: &quot;, with( admits_test, mean( abs( COVID_NEW_ADM_CNT - D1_admissions), na.rm=TRUE)) ) ## Mean absolute error for the boosted forest model: 1.644679 ## Mean absolute error for the lag-1 observations: 2.081395 # calculate poisson predictive intervals admits_ci = data.frame( lower=qpois(0.25, admits_preds), upper=qpois(0.75, admits_preds)) # plot the daily admissions in the validation data with(admits_test, plot( date, D1_admissions )) #add a line for the predictions out of the model lines( admits_test$date, admits_preds, col=&#39;red&#39;) #add lines for the prediction 50% interval lines( admits_test$date, admits_ci$lower, col=&#39;blue&#39;) lines( admits_test$date, admits_ci$upper, col=&#39;blue&#39;) # calculate coverage of the prediction interval cat( &quot;Prediction interval coverage (nominal 50%): &quot;, mean(admits_test$D1_admissions &gt;= admits_ci$lower &amp; admits_test$D1_admissions &lt;= admits_ci$upper, na.rm=TRUE )) ## Prediction interval coverage (nominal 50%): 0.5852713 Our boosted forest model has lower prediction error than the lag-1 observations, but the coverage of the prediction interval is 58% - that’s somewhat greater than the nominal 50%. This model isn’t perfect, but the predictive performance is good and validation doesn’t reveal any terrible behavior. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
