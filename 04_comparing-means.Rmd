```{r ch4-loads, echo=FALSE, warning=FALSE, include=FALSE}
library("readr")
library( "lubridate" )
library( "dplyr" )
library( "gbm" )

load("output/gbsg.rda" )
covid = read_csv( "data/covid.csv" )
covid$DAY_OF_THE_WEEK = factor(covid$DAY_OF_THE_WEEK)
```

# Validating the model's functional form
We can validate the model's functional form by making predictions and comparing them to the truth. If the estimated mean function is valid, then the observed should be in line with predictions. Of course, there will stil be some error, but the predictions should be accurate on average.


## Posit a model
To posit a model here means to select a model form and distribution, and apply them to a piece of estimation software. Most often the model form is a generalized linear model (which includes linear regression as a special case), but we are also looking at random forest-type models, which are not linear in their inputs.

### Model form
Selecting a model form includes deciding which covariates have a predictive relationship with the response, and whether their relationship is linear, linear following some transformation, or should be modeled more flexibly, e.g. with a tree-based approach.

### Model distribution
Knowing the allowable range of the response will help you to decide what distribution to specify in fitting a model. Here are some common, crucial decision points:

- If the response is in the form of "number of events out of some number of tries" (e.g. a basketball player's free throw percentage is the number of made free throws out of tries), then a binomial distribution may be appropriate. This is also the case when the number of "tries" is one - as in our example of five years survival for women with breast cancer. Here, the "event" is "the woman survives five years after diagnosis", and each person in the study gets one try.
- If the response is a count of independent events, with no "out of" tries, then it may be Poisson distributed.
- If the data are continuous with no bounds then they may be normally distributed.
- If the data are continuous, but have a defined lower bound at zero then they may follow a Gamma distribution.


## Validate the assumptions
Having posited a model, we may check whether the model generates fits and predictions that agree with the assumptions. Let's do this for our examples.

```{r mean-validation-cars, cache=TRUE}
nfold = 5
folds = sample( 1:nfold, size=nrow(cars), replace=TRUE )

preds = list( 'a' = numeric(nrow(cars)), 'b' = numeric(nrow(cars)), 'c' = numeric(nrow(cars)), 'd' = numeric(nrow(cars)), 'e' = numeric(nrow(cars)), 'f'= numeric(nrow(cars)))

for ( i in 1:nfold ) {
  # Estimate a model for the stopping distance data
  lm1 = lm( dist ~ speed, data=cars[ folds!= i, ] )
  lm2 = lm( dist ~ I(speed^2), data=cars[ folds!= i, ] )
  lm3 = lm( dist ~ I(speed^3), data=cars[ folds!= i, ] )
  lm4 = lm( dist ~ I(speed^2) + 0, data=cars[ folds!= i, ] )
  
  lm5 = lm( dist ~ I(speed^2) + speed, data=cars[ folds!= i, ] )
  lm6 = lm( dist ~ I(speed^2) + speed+0, data=cars[ folds!= i, ] )
    
  
  preds[['a']][ folds == i ] = predict( lm1, cars[ folds==i, ] )
  preds[['b']][ folds == i ] = predict( lm2, cars[ folds==i, ] )
  preds[['c']][ folds == i ] = predict( lm3, cars[ folds==i, ] )
  preds[['d']][ folds == i ] = predict( lm4, cars[ folds==i, ] )
  preds[['e']][ folds == i ] = predict( lm5, cars[ folds==i, ] )
  preds[['f']][ folds == i ] = predict( lm6, cars[ folds==i, ] )
}

# calculate the mean squared error of the models
var( preds[['a']] - cars$dist )
var( preds[['b']] - cars$dist )
var( preds[['c']] - cars$dist )
var( preds[['d']] - cars$dist )
var( preds[['e']] - cars$dist )
var( preds[['f']] - cars$dist )
```


```{r mean-validation-gbsg, cache=TRUE}
nfold = 5
fold = sample( 1:nfold, size=nrow(gbsg), replace=TRUE )

pred_gbsg = numeric( nrow(gbsg) )

for ( i in 1:nfold ) {
  # Estimate a model for the stopping distance data
  glm1 = glm( fys ~ age + meno + size + grade + log(nodes) + sqrt(pgr) + sqrt(er) + hormon, data=gbsg[ fold != i, ], family='binomial' )
  
  # get predictions for the left-out CV fold
  pred_gbsg[ fold == i ] = predict( glm1, gbsg[ fold == i, ], type='response' )
}

# calculate the mean squared error of the models
var( (pred_gbsg - gbsg$fys) / sqrt(pred_gbsg * (1-pred_gbsg)), na.rm=TRUE )
```


#### COVID-19 admissions example

Recall that one of our first validation tasks for the COVID-19 admissions data was to check whether the time series was autocorrelated. We concluded that there was not an important amount of autocorrelation, and so we will use modeling methods that treat the data as independent. 

Despite that, when the data are a time series, it is good practice for validation splits to respect the time ordering of the observations. After all, you need to be concerned about independence of the predictors as well as the response, and it is simply more realistic to validate the model with a known past and unknown future. That means setting a split date, then training over past data and predicting future observations.

You can still use cross-validation for time-series data - it is called walkforward validation, where each fold is a contiguous temporal block. Here, though, we use a single  train/test split: train a model using all data prior to July, 2021 and then use it to predict COVID admissions since then.

```{r mean-validation-covid, cache=TRUE}
set.seed(20211108)

# create a variable for the one-day ahead admissions:
covid$D1_admissions = lead(covid$COVID_NEW_ADM_CNT, 1)

# remove any rows that have NA values
covid = covid[ rowSums( is.na(covid)) == 0, ]

# establish the split date and use it to define a training set:
split = "2021-07-01"
train = covid$date < split

# use CV on the training data to decide which variables to use in the models
screen = gbm( D1_admissions ~ .,
              data=covid[ train, -1],
              n.trees=3000,
              distribution="poisson",
              interaction.depth=5,
              n.minobsinnode=3,
              shrinkage=0.0025,
              cv.folds=5 )

# show the most influential variables for predicting COVID admissions:
my_summary = summary(screen,
                     plotit=FALSE,
                     n.trees = gbm.perf(screen, plot.it=FALSE) )
print(my_summary)

# use the top twenty variables from the screening model
f = paste0("D1_admissions ~ ", paste( my_summary$var[1:20], collapse=" + "))

# Estimate a model for the covid admissions data
boost = gbm( formula(f),
             data=covid[ train, ],
             n.trees=3000,
             distribution="poisson",
             interaction.depth=5,
             n.minobsinnode=3,
             shrinkage=0.0025,
             cv.folds=5 )

# make predictions from the covid admissions model
pred_covid = predict(boost, covid, type='response')[ !train ]

# plot the predicted vs. observed
plot(pred_covid, covid$D1_admissions[!train], xlab="prediction", ylab="actual future admissions")
abline(a=0, b=1)
```

The predictions are clearly somewhat muted relative to the actual future admissions, but they do capture the trend and when we look at the admissions from the perspective of the assumed Poisson distribution, we will see that the wider errors toward the right of the plot are expected.


```{r save-output, echo=FALSE, include=FALSE, message=FALSE, warning=FALSE}
save( pred_gbsg, file="output/pred_gbsg.rda")
save( gbsg, file="output/gbsg.rda")
save( pred_covid, file="output/pred_covid.rda")
```


